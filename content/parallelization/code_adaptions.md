# Example Walkthrough

Even in an embarassingly parallel situation we will have to make a couple of adaptions to the code
in order to be able to run parallel executions of our code. For this walkthrough we will be starting with
a jupyter notebook that is based on the
[Nearest Neighbor Classification example of the scikit-learn toolkit](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html).

## Convert the notebook to a python script

The first step is to convert the notebook into a python script. This is rther simple and can be done in jupyter by going to:

```
"File" -> "Save and Export Notebook as..." -> "Executable Script"
```

The result of this conversion can be found [here](/code/python/scikit_example/knn_iris.py).

### Split into a pre-processing and a execution script

Our code has two distinct parts, a pre-processing part and a model generation and plotting part.
The former part needs to be run exactly once (and actually shouldn't be run separately each time if we
want) to compare the results, as the training/test split should be the same for all methods.
Thus, we split our code into two files, `preprocess.py` and `train_and_plot.py`.

```{literalinclude} /code/python/scikit_example/preprocess.py
    :language: python
    :emphasize-lines: 1-7, 53
```

We only include those imports necessary and make sure, that the data/preprocessed folder exists when we run the code

```{literalinclude} /code/python/scikit_example/train_and_plot.py
    :language: python
    :emphasize-lines: 1-9
```

This allows us to run the pre-processing once and in further steps always use the already pre-processed
data avoiding unnecessary compute time if we e.g. want to change the metrics.

## Update code to run on a cluster

To run the code on a cluster we will need two steps, first, we will need to create an environment in
which the code can run. How you go about this depends on the cluster, but most clusters allow
the use of containers, which is why we will be using a container for this example. In the second step, we need to execute our code on the cluster using the scheduler.

### Build a container for dependencies

We assume, that your cluster does have support for singularity. We provide both a singularity and
docker definition file. The finished container can be found [here](TODO). Download the container on
your cluser using:

```bash
# You might need to activate singularity depending on your cluster
singularity build python3_10 docker://harbor.cs.aalto.fi/aaltorse-public/parallel-workflow:latest
```

This commands builds the singularity container based on the docker image we provide. Containers are discussed in more details in our [Container Lecture](TODO)

### Create a slurm script to run the code

We will need a slurm script to submit our job to the cluster queue. The script we will be using is the
following:

```{literalinclude} /code/slurm/scikit_example/submit_job.sh
    :language: slurm

```

When this is done, we now have some code that runs on the cluster. This code will run all the different
metrics neighbourhood sizes one after another.

## Updating the code for parallel execution (and flexibility)

Slurm offers a type of job, called array job, which essentially runs the same lines of code multiple
times. The only difference between one job and the next is a environment variable `SLURM_ARRAY_TASK_ID`.
This variable is different and takes on the values given in the `--array` parameter provided to the job.
Our code currently has hard coded input data and hard coded parameters. If we want to scan different
parameters, we will always have to change our code. This means we have to adapt our code such, that we
select the parameter based on some input to our code.
In order to do so, we have to provide the parameters we want to supply to the model
If we want to run different combinations in parallel, it would be preferable, if we can provide the data
from some input argument. There are many options to do so, but we will use `argparse` in our example (some more details can be found [here](https://aaltoscicomp.github.io/python-for-scicomp/scripts/#parsing-command-line-arguments-with-argparse)).

```{literalinclude} /code/python/scikit_example/array/train_and_plot.py
    :language: python
    :emphasize-lines: 2, 24-38, 43

```

This file now reads in a parameter file (either a default one, or one that we provide) and extracts the
`n_neighbours` parameter from that file. This file can be generated by the following script:

```{literalinclude} /code/python/scikit_example/array/create_parameter_array.py
    :language: python
    :emphasize-lines: 2, 24-38, 43

```

This parameter creation script is computationally very undemanding and can likely be run on any login node, but you can also wrap it into a slurm job.

### Update the slurm script

We now have a two-step process, so we need to first do the pre-processing of our data in one job, which can be submitted using the following job:

```{literalinclude} /code/slurm/scikit_example/submit_preprocess.sh
    :language: slurm

```

and then run the array job for our actual computation

```{literalinclude} /code/slurm/scikit_example/submit_parallel.sh
    :language: slurm
    :emphasize-lines: 3, 9

```

Note, that we use a sbatch array from 0 to 6, since we have 7 values in our neighbours array and python starts indexing from 0.
There are two ways how you can submit these jobs and ensure, that they work properly:

- You can first submit the preprocess job, and wait till it finishes before submitting the computation  
  jobs
- You can submit the preprocess jobs and directly submit the computing jobs giving the pre-processing job
  as a dependency: `sbatch submit_parallel.sh --dependency=afterok:<job_id_of_the_preprocessing_task>`
  Make sure, that you replace the job_id by your actual pre-processing job id, which you will be told,
  when submitting the pre-processing job

## Post processing steps

While not relevant to our example, in many cases, you will generate many output files, that you need to
combine again after the computation is done (e.g. to compute some general statistics).
In this situation, you need to think about how the result data from each run can be stored.
A simple example of this could be (in pseudo-code):

```python
data = load_preprocessed_data()
for i in 1..20:
    res[i] = compute(data[i])
calc_statistics(res)
```

If you parallelize this code you will end up with something along the lines of:

```python
i = index_from_arguments()
data = load_preprocessed_data()
res = compute(data[i])
with open(f'outputs/result_{i}') as f:
    pickle.dump(f, res)
```

which saves the data to a file in the results folder
You can then use the following code to collect these results into one large file, (or directly process
it instead of saving it again)

```{literalinclude} /code/python/collection.py
    :language: python

```

## Exercise 1

````{exercise} Parallel-1: Make the script accept an input index
Our example is a very simple code which extracts cities for a country from a list of
cities with their countries and writes a list of these cities.
The data for the cities for this example are provided in the {download}`cities.csv </code/cities.csv>` file.
The code itself is provided {download}`here </code/python/long_code.py>`:
```{literalinclude} /code/python/long_code.py
    :language: python
```

Adapt the code, such that it takes an integer input and generates one country list
for each integer input. In this instance, the order of execution is not important
as one list per country will be produced.

````

````{solution} Solution: Parallel-1
The simplest solution is to use `sys.argv` taking in the first argument and converting
it to an integer. You can also use more elaborate input parsers (see for example
[this lecture about argument parsing](https://aaltoscicomp.github.io/python-for-scicomp/scripts/#parsing-command-line-arguments-with-argparse))

```{literalinclude} /code/python/long_code_for_index.py
    :language: python
    :emphasize-lines: 1, 30-37
```

````
